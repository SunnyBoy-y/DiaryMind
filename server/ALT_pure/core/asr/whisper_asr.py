from faster_whisper import WhisperModel
import os


def audio_to_text(audio_path, model_size="medium", language=None, device="cpu"):
    """
    å°†æœ¬åœ°éŸ³é¢‘æ–‡ä»¶è½¬ä¸ºæ–‡æœ¬ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰

    å‚æ•°ï¼š
    - audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ mp3, wav, m4a ç­‰å¸¸è§æ ¼å¼ï¼‰
    - model_size: æ¨¡å‹å¤§å°ï¼Œå¯é€‰: 'tiny', 'base', 'small', 'medium', 'large-v2', 'large-v3'
    - language: è¯­è¨€ä»£ç ï¼Œå¦‚ 'zh' ä¸­æ–‡, 'en' è‹±æ–‡ï¼Œè®¾ä¸º None å¯è‡ªåŠ¨æ£€æµ‹
    - device: è¿è¡Œè®¾å¤‡ï¼Œ'cuda'ï¼ˆGPUï¼‰ã€'cpu'ï¼Œ'auto' è‡ªåŠ¨é€‰æ‹©

    è¿”å›ï¼š
    - è¯†åˆ«å‡ºçš„æ–‡æœ¬å­—ç¬¦ä¸²
    """

    # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°: {audio_path}")

    # åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹åˆ°ç¼“å­˜ç›®å½•ï¼‰
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ '{model_size}'...")

    # å¤„ç† CUDA/cuDNN é—®é¢˜ï¼Œå¦‚æœ GPU ä¸å¯ç”¨åˆ™å›é€€åˆ° CPU
    try:
        if device == "auto":
            # æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨
            import torch
            if torch.cuda.is_available():
                device = "cuda"
                compute_type = "float16"
            else:
                device = "cpu"
                compute_type = "int8"
        elif device == "cuda":
            compute_type = "float16"
        else:
            compute_type = "int8"

        model = WhisperModel(model_size, device=device, compute_type=compute_type)
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    except Exception as e:
        print(f"GPU åŠ é€Ÿä¸å¯ç”¨ï¼Œå›é€€åˆ° CPU: {e}")
        model = WhisperModel(model_size, device="cpu", compute_type="int8")
        print("ä½¿ç”¨è®¾å¤‡: cpu")

    print(f"å¼€å§‹è¯†åˆ«éŸ³é¢‘: {audio_path}")
    segments, info = model.transcribe(
        audio_path,
        language=language,  # å¯è®¾ä¸º 'zh', 'en'ï¼Œæˆ– None è‡ªåŠ¨æ£€æµ‹
        beam_size=5,  # æŸæœç´¢å¤§å°ï¼Œæé«˜ç²¾åº¦
        best_of=5,
        temperature=0.0,  # å›ºå®šæ¸©åº¦æå‡ç¨³å®šæ€§
        vad_filter=True,  # å¯ç”¨é™éŸ³è¿‡æ»¤ï¼Œæå‡é•¿éŸ³é¢‘æ•ˆç‡
        vad_parameters=dict(min_silence_duration_ms=500)
    )

    # è¾“å‡ºæ£€æµ‹åˆ°çš„è¯­è¨€
    detected_lang = info.language
    print(f"æ£€æµ‹åˆ°çš„è¯­è¨€: {detected_lang} (ç½®ä¿¡åº¦: {info.language_probability:.2f})")

    # æ‹¼æ¥æ‰€æœ‰æ–‡æœ¬æ®µ
    text = "".join(segment.text for segment in segments)
    return text.strip()


# ================== ä½¿ç”¨ç¤ºä¾‹ ==================
if __name__ == "__main__":
    audio_file = r"C:\Users\13600\Desktop\ALT_pure\core\asr\temp\record_audio_16e7d605fc7749328084981bf2fb2bd9.wav"  # âœ… æ›¿æ¢ä¸ºä½ çš„éŸ³é¢‘è·¯å¾„

    try:
        result = audio_to_text(
            audio_path=audio_file,
            model_size="medium",  # ä½¿ç”¨ medium æ¨¡å‹ä»¥å‡å°‘èµ„æºæ¶ˆè€—
            language=None,  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€ï¼ˆé€‚åˆä¸­è‹±æ··åˆï¼‰
            device="gpu"  # æ˜ç¡®æŒ‡å®šä½¿ç”¨ CPU
        )
        print("\nğŸ“ è¯†åˆ«ç»“æœï¼š")
        print(result)
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
